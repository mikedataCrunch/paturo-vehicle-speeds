{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2142f0fd",
   "metadata": {},
   "source": [
    "# SHAP interpretation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3ecb1e7d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-27T14:33:01.996007Z",
     "start_time": "2023-06-27T14:33:01.960032Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAM: 251.54 GB\n"
     ]
    }
   ],
   "source": [
    "import shap\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import lightgbm as lgb\n",
    "import pickle\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    r2_score, max_error, mean_absolute_error, mean_squared_error,\n",
    "    mean_absolute_percentage_error, make_scorer\n",
    ")\n",
    "from sklearn.model_selection import (\n",
    "    TimeSeriesSplit, train_test_split, cross_val_score)\n",
    "\n",
    "from glob import glob\n",
    "import os\n",
    "import optuna\n",
    "import joblib\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "import psutil\n",
    "    \n",
    "src_date = \"2023-06-17\"     \n",
    "\n",
    "ram_gb = psutil.virtual_memory().total / 2**30 # total physical memory in bytes\n",
    "print(f\"RAM: {ram_gb:.2f} GB\")\n",
    "\n",
    "def _change_bf_name(name):\n",
    "    rename_map = {\n",
    "        'all': '50-1000',\n",
    "        '50-50' : '50',\n",
    "        '100-100': '100',\n",
    "        '150-150': '150',\n",
    "        '200-200': '200',\n",
    "        '250-250': '250',\n",
    "        '500-500': '500',\n",
    "        '1000-1000': '1000',\n",
    "    }\n",
    "    return rename_map.get(name, name)\n",
    "\n",
    "\n",
    "# preprocess utils\n",
    "\n",
    "# compress dtypes\n",
    "dtypes_compressed = {\n",
    "    'datetime': np.object_,    \n",
    "    'hour': np.int8,\n",
    "    'month': np.int8,\n",
    "    'dayofweek': np.int8,\n",
    "    'number_of_lanes': np.float32,\n",
    "    \n",
    "    'speed_limit_kph': np.float32,    \n",
    "    \n",
    "    'lon': np.float64,\n",
    "    'lat': np.float64,\n",
    "    'elevation': np.int16,    \n",
    "    'agg_speed': np.float64,  \n",
    "    \n",
    "    'pix_business_4x4_200': np.int16,\n",
    "    'pix_residential_4x4_200': np.int16,\n",
    "    'pix_industrial_4x4_200': np.int16,\n",
    "    'pix_institutional_4x4_200': np.int16,\n",
    "    'pix_business_4x4_50': np.int16,\n",
    "    'pix_residential_4x4_50': np.int16,\n",
    "    'pix_industrial_4x4_50': np.int16,\n",
    "    'pix_institutional_4x4_50': np.int16,\n",
    "    'pix_business_4x4_100': np.int16,\n",
    "    'pix_residential_4x4_100': np.int16,\n",
    "    'pix_industrial_4x4_100': np.int16,\n",
    "    'pix_institutional_4x4_100': np.int16,\n",
    "    'pix_business_4x4_150': np.int16,\n",
    "    'pix_residential_4x4_150': np.int16,\n",
    "    'pix_industrial_4x4_150': np.int16,\n",
    "    'pix_institutional_4x4_150': np.int16,\n",
    "    'pix_business_4x4_250': np.int16,\n",
    "    'pix_residential_4x4_250': np.int16,\n",
    "    'pix_industrial_4x4_250': np.int16,\n",
    "    'pix_institutional_4x4_250': np.int16,\n",
    "    'pix_business_4x4_500': np.int16,\n",
    "    'pix_residential_4x4_500': np.int16,\n",
    "    'pix_industrial_4x4_500': np.int16,\n",
    "    'pix_institutional_4x4_500': np.int16,\n",
    "    'pix_business_4x4_1000': np.int16,\n",
    "    'pix_residential_4x4_1000': np.int16,\n",
    "    'pix_industrial_4x4_1000': np.int16,\n",
    "}\n",
    "\n",
    "def correct_service_rd_kphlimit(temp_df):\n",
    "    temp_df.loc[temp_df['speed_limit_kph'] == 36.7, 'speed_limit_kph'] = 20\n",
    "    return temp_df\n",
    "\n",
    "def impute_lanes(df):\n",
    "    df['number_of_lanes'] = df['number_of_lanes'].replace(np.nan, value=2)\n",
    "    return df\n",
    "\n",
    "def filter_brgy(df):\n",
    "    df['barangay'] = df['barangay'].fillna('Out-of-town')\n",
    "    \n",
    "    # not in cauayan boundary\n",
    "    df = df.loc[df['barangay'] != 'Out-of-town'] \n",
    "    return df\n",
    "\n",
    "def filter_num_periods(df, thresh):\n",
    "    df = df.loc[df['num_periods'] >= thresh]\n",
    "    return df\n",
    "\n",
    "def filter_vehicles(df, remove_list=None):\n",
    "    df = df.loc[~df.vehicle_id.isin(remove_list)]\n",
    "    return df\n",
    "\n",
    "def prepare_df(df):\n",
    "    df = df.astype(dtype=dtypes_compressed)\n",
    "    \n",
    "    # vehicles with erratic sending of data; stopped sending data before collection date\n",
    "    remove_list = [\n",
    "        123200872653, 123200872678, \n",
    "        123200872713, 123200872727, 123200872819,\n",
    "    ] \n",
    "    df = filter_brgy(df)\n",
    "    df = filter_vehicles(df, remove_list=remove_list)\n",
    "    df = filter_num_periods(df, thresh=12)\n",
    "    df = impute_lanes(df)\n",
    "    df = correct_service_rd_kphlimit(df)\n",
    "    df = df.drop_duplicates()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6e976ab4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-27T14:33:06.421138Z",
     "start_time": "2023-06-27T14:33:02.519933Z"
    }
   },
   "outputs": [],
   "source": [
    "# SANITY CHECK\n",
    "from itertools import product\n",
    "\n",
    "buffer_ablation = [\n",
    "    'all',\n",
    "    [50, 100, 150, 200, 250, 500],\n",
    "    [50, 100, 150, 200, 250],\n",
    "    [50, 100, 150, 200],\n",
    "    [50, 100, 150],\n",
    "    [50, 100],\n",
    "    [1000],\n",
    "    [500],\n",
    "    [250],\n",
    "    [200],\n",
    "    [150],\n",
    "    [100],\n",
    "    [50], \n",
    "]\n",
    "\n",
    "\n",
    "etypes = [\n",
    "    \"all_features\",\n",
    "    \"landuse_and_time\", # queue this with the ablation\n",
    "    \"landuse_only\",\n",
    "]\n",
    "\n",
    "exp_combis = product(etypes, buffer_ablation)\n",
    "\n",
    "fixed_params = {\n",
    "    'tree_learner': 'data',\n",
    "    'seed':11,\n",
    "    'verbose': -1,\n",
    "    'boosting_type': 'goss',    \n",
    "}\n",
    "\n",
    "best_param_dict = {}\n",
    "\n",
    "for combi in exp_combis:\n",
    "    experiment_type = combi[0]\n",
    "    buffers = combi[1]\n",
    "    \n",
    "    study_name = f'{experiment_type}'\n",
    "    \n",
    "    if buffers=='all':\n",
    "        buffers = buffers\n",
    "    else:\n",
    "        buffers = \"-\".join([str(buffers[0]), str(buffers[-1])])\n",
    "\n",
    "    optuna_dir = f'../experiments/post-review/{buffers}/{src_date}'\n",
    "    optuna_path = os.path.join(optuna_dir, f\"{study_name}.db\")\n",
    "#     print(\"OPTUNA PATH: \", optuna_path)\n",
    "    \n",
    "    if not os.path.exists(optuna_path):\n",
    "        print(f\"NO OPTUNA STUDY: {buffers}-{experiment_type}\")\n",
    "    \n",
    "    study_name = combi[0] # expriement type\n",
    "    study = optuna.load_study(\n",
    "        study_name=study_name, \n",
    "        storage=f\"sqlite:///{optuna_path}\"\n",
    "    )\n",
    "    params = study.best_params\n",
    "    params.update(fixed_params)\n",
    "    \n",
    "    feature_set = combi[0]\n",
    "    buffer_range = _change_bf_name(buffers) # combi[1]\n",
    "    \n",
    "    best_param_dict[f\"{feature_set}-{buffer_range}\"] = params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b7bc3692",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-27T14:33:18.968246Z",
     "start_time": "2023-06-27T14:33:06.424731Z"
    }
   },
   "outputs": [],
   "source": [
    "# load data\n",
    "src_dir = f\"../datasets/processed/post-review-tt-splits/{src_date}\"\n",
    "train = pd.read_csv(os.path.join(src_dir, \"train.csv\"))\n",
    "train = prepare_df(train)\n",
    "\n",
    "test = pd.read_csv(os.path.join(src_dir, \"test.csv\"))\n",
    "test = prepare_df(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "52b4e48d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-27T15:04:59.163441Z",
     "start_time": "2023-06-27T14:33:18.972081Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[50, 100, 150, 200, 250, 500, 1000]\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=207, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=207\n",
      "SHAP res shape:  (266751, 33)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "1it [08:16, 496.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[50, 100, 150, 200, 250, 500, 1000]\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=260, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=260\n",
      "SHAP res shape:  (266751, 28)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "2it [27:30, 883.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[50, 100, 150, 200, 250, 500, 1000]\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=101, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=101\n",
      "SHAP res shape:  (266751, 30)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3it [31:40, 633.39s/it]\n"
     ]
    }
   ],
   "source": [
    "# feature\n",
    "feature_sets = ['all_features', 'landuse_only', 'landuse_and_time']\n",
    "buffer_ranges = ['50-1000']\n",
    "\n",
    "shap_explanations = []\n",
    "for combi in tqdm(product(feature_sets, buffer_ranges)):\n",
    "    feature_set = combi[0]\n",
    "    buffer_range = combi[1]\n",
    "\n",
    "    # target\n",
    "    target = 'agg_speed'\n",
    "\n",
    "    # features\n",
    "    time_cols = ['hour', 'dayofweek']\n",
    "\n",
    "    road_cols = [\n",
    "        'number_of_lanes', 'speed_limit_kph', 'elevation',\n",
    "    ]\n",
    "    buffer_sizes = [50, 100, 150, 200, 250, 500, 1000]\n",
    "    \n",
    "    if len(buffer_range.split('-'))==1:\n",
    "        buffer_sizes = [int(buffer_range.split('-')[0])]\n",
    "    else:\n",
    "        min_size_index = buffer_sizes.index(\n",
    "            int(buffer_range.split('-')[0])\n",
    "        )\n",
    "        max_size_index = buffer_sizes.index(\n",
    "            int(buffer_range.split('-')[1])\n",
    "        )\n",
    "        buffer_sizes = buffer_sizes[min_size_index:max_size_index + 1]\n",
    "    print(buffer_sizes)\n",
    "    \n",
    "    landuse_cols = []\n",
    "    for size in buffer_sizes:\n",
    "        landuse_cols.extend(\n",
    "            [\n",
    "                f'pix_residential_4x4_{size}',\n",
    "                f'pix_institutional_4x4_{size}',\n",
    "                f'pix_industrial_4x4_{size}',\n",
    "                f'pix_business_4x4_{size}',\n",
    "            ]\n",
    "        )\n",
    "    cat_cols = time_cols\n",
    "\n",
    "    if feature_set == \"all_features\":\n",
    "        features = time_cols + road_cols + landuse_cols\n",
    "        cat_cols_index = [\n",
    "            index for (index, col) in enumerate(features) if col in cat_cols]\n",
    "\n",
    "    elif feature_set == \"landuse_and_time\":\n",
    "        features = time_cols + landuse_cols\n",
    "        cat_cols_index = [\n",
    "            index for (index, col) in enumerate(features) if col in cat_cols]\n",
    "\n",
    "    else:\n",
    "        features = landuse_cols\n",
    "        cat_cols_index = None    \n",
    "    \n",
    "    \n",
    "    params = best_param_dict[f\"{feature_set}-{buffer_range}\"]\n",
    "    \n",
    "    X_train, y_train = train[features], train[target]\n",
    "    X_test, y_test = test[features], test[target]\n",
    "    \n",
    "    model = lgb.LGBMRegressor(**params)\n",
    "    model.fit(X_train, y_train, categorical_feature=cat_cols_index)\n",
    "    \n",
    "    preds = model.predict(X_test)\n",
    "    true = y_test.values\n",
    "    \n",
    "    final_results = {\n",
    "        \"y_true\": list(true),\n",
    "        \"y_pred\": list(preds),\n",
    "        \"feature_list\": X_test.columns.tolist(),\n",
    "    }\n",
    "    \n",
    "    explainer = shap.TreeExplainer(model)\n",
    "    shap_values = explainer(X_test)\n",
    "    print(\"SHAP res shape: \", shap_values.shape)\n",
    "    \n",
    "    final_results[\"shap_values\"] = shap_values\n",
    "    \n",
    "    save_dir = f\"../experiments/post-review/shap/{src_date}\"\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    \n",
    "    save_fname = os.path.join(save_dir, f\"{feature_set}-{buffer_range}-shap.pickle\")\n",
    "    with open(save_fname, 'wb') as handle:\n",
    "        pickle.dump(final_results, handle)\n",
    "        \n",
    "    shap_explanations.append(final_results) # for plotting in this notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c33e3cb",
   "metadata": {},
   "source": [
    "## End"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
