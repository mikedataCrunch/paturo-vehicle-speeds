{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9f2a2ae7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-24T16:59:15.526996Z",
     "start_time": "2023-05-24T16:59:14.531088Z"
    }
   },
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "import os\n",
    "from statsmodels.stats.multitest import multipletests\n",
    "\n",
    "from itertools import combinations\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import plot_init as pu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4d072b05",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-24T16:59:15.548349Z",
     "start_time": "2023-05-24T16:59:15.530096Z"
    }
   },
   "outputs": [],
   "source": [
    "def calculate_spearmanr(arr1, arr2):\n",
    "        \"\"\"Calculate spearman rank correlation\"\"\"\n",
    "        temp_df = pd.DataFrame()\n",
    "        temp_df['arr1'] = arr1\n",
    "        temp_df['arr2'] = arr2\n",
    "        temp_df = temp_df.dropna()\n",
    "        \n",
    "        arr1 = temp_df['arr1']\n",
    "        arr2 = temp_df['arr2']\n",
    "        \n",
    "        corr, pval = stats.spearmanr(arr1, arr2)\n",
    "        return corr, pval, corr**2 # effect size   \n",
    "    \n",
    "def calculate_chi(arr1, arr2):\n",
    "        \"\"\"Calculate Chi-square test of independence\"\"\"\n",
    "        temp_df = pd.DataFrame()\n",
    "        temp_df['arr1'] = arr1\n",
    "        temp_df['arr2'] = arr2\n",
    "        temp_df = temp_df.dropna()\n",
    "        \n",
    "        arr1 = temp_df['arr1']\n",
    "        arr2 = temp_df['arr2']\n",
    "        \n",
    "        row_count = arr1.nunique()\n",
    "        col_count = arr2.nunique()\n",
    "\n",
    "\n",
    "        contingency = pd.crosstab(arr1, arr2)\n",
    "        n = contingency.sum().sum()\n",
    "        \n",
    "        # Chi-square test of independence.\n",
    "        corr, pval, dof, expected = stats.chi2_contingency(\n",
    "            contingency, correction=False, lambda_=None)\n",
    "        \n",
    "        # returns chi2, p_uncorrected, dof, expected, num_rows, num_cols, n\n",
    "        return corr, pval, dof, expected, row_count, col_count, n\n",
    "    \n",
    "def bergsma_corrected_cramers_v2(chi2, row_count, col_count, n):\n",
    "    \"\"\"Calculate cramers_v2_corrected\"\"\"\n",
    "    \n",
    "    phi2 = chi2/n\n",
    "    phi2_corrected = phi2 - (1/(n-1))*(row_count-1)*(col_count-1)\n",
    "    phi2_corrected_nonneg = max(0, phi2_corrected)\n",
    "\n",
    "    row_count_corrected = row_count - (1/(n-1))*(row_count-1)**2\n",
    "    col_count_corrected = col_count - (1/(n-1))*(col_count-1)**2\n",
    "\n",
    "    cramers_v2_corrected = phi2_corrected_nonneg/min(row_count_corrected-1, col_count_corrected-1)\n",
    "\n",
    "    return cramers_v2_corrected # effect size\n",
    "\n",
    "def kw_dunn(groups, to_compare=None, alpha=0.05, method='bonf'):\n",
    "    \"\"\"\n",
    "    Kruskal-Wallis 1-way ANOVA with Dunn's multiple comparison test\n",
    "    Arguments:\n",
    "    ---------------\n",
    "    groups: sequence, pd.Series\n",
    "        arrays corresponding to k mutually independent samples from\n",
    "        continuous populations\n",
    "        For pd.Series inputs,  index is group (category) and values are arrays\n",
    "        of numerical var values belonging to that group.\n",
    "    to_compare: sequence\n",
    "        tuples specifying the indices of pairs of groups to compare, e.g.\n",
    "        [(0, 1), (0, 2)] would compare group 0 with 1 & 2. by default, all\n",
    "        possible pairwise comparisons between groups are performed.\n",
    "    alpha: float\n",
    "        family-wise error rate used for correcting for multiple comparisons\n",
    "        (see statsmodels.stats.multitest.multipletests for details)\n",
    "    method: string\n",
    "        method used to adjust p-values to account for multiple corrections (see\n",
    "        statsmodels.stats.multitest.multipletests for options)\n",
    "    Returns:\n",
    "    ---------------\n",
    "    H: float\n",
    "        Kruskal-Wallis H-statistic\n",
    "    p_omnibus: float\n",
    "        p-value corresponding to the global null hypothesis that the medians of\n",
    "        the groups are all equal\n",
    "    Z_pairs: float array\n",
    "        Z-scores computed for the absolute difference in mean ranks for each\n",
    "        pairwise comparison\n",
    "    p_corrected: float array\n",
    "        corrected p-values for each pairwise comparison, corresponding to the\n",
    "        null hypothesis that the pair of groups has equal medians. note that\n",
    "        these are only meaningful if the global null hypothesis is rejected.\n",
    "    reject: bool array\n",
    "        True for pairs where the null hypothesis can be rejected for the given\n",
    "        alpha\n",
    "    Reference:\n",
    "    ---------------\n",
    "    Gibbons, J. D., & Chakraborti, S. (2011). Nonparametric Statistical\n",
    "    Inference (5th ed., pp. 353-357). Boca Raton, FL: Chapman & Hall.\n",
    "    \"\"\"\n",
    "\n",
    "    # omnibus test (K-W ANOVA)\n",
    "    # -------------------------------------------------------------------------\n",
    "\n",
    "    groups = [np.array(gg) for gg in groups]\n",
    "\n",
    "    k = len(groups)\n",
    "\n",
    "    n = np.array([len(gg) for gg in groups])\n",
    "\n",
    "    if np.any(n < 5):\n",
    "        warnings.warn(\"Sample sizes < 5 are not recommended (K-W test assumes \"\n",
    "                      \"a chi square distribution)\")\n",
    "\n",
    "    allgroups = np.concatenate(groups)\n",
    "    N = len(allgroups)\n",
    "\n",
    "    ranked = stats.rankdata(allgroups)\n",
    "\n",
    "    # correction factor for ties\n",
    "    T = stats.tiecorrect(ranked)\n",
    "    if T == 0:\n",
    "        raise ValueError('All numbers are identical in kruskal')\n",
    "\n",
    "\n",
    "    # sum of ranks for each group\n",
    "    j = np.insert(np.cumsum(n), 0, 0)\n",
    "    R = np.empty(k, dtype=float)\n",
    "    for ii in range(k):\n",
    "        R[ii] = ranked[j[ii]:j[ii + 1]].sum()\n",
    "\n",
    "\n",
    "    # the Kruskal-Wallis H-statistic\n",
    "    H = ((12. / (N * (N + 1.))) \n",
    "         * ((R ** 2.) / n).sum() - 3 * (N + 1))\n",
    "\n",
    "    # apply correction factor for ties\n",
    "    H /= T\n",
    "\n",
    "    df_omnibus = k - 1\n",
    "    p_omnibus = stats.distributions.chi2.sf(H, df_omnibus)\n",
    "\n",
    "    # multiple comparisons\n",
    "    # -------------------------------------------------------------------------\n",
    "    # by default we compare every possible pair of groups\n",
    "    if to_compare is None:\n",
    "        to_compare = tuple(combinations(range(k), 2))\n",
    "\n",
    "    ncomp = len(to_compare)\n",
    "\n",
    "    Z_pairs = np.empty(ncomp, dtype=float)\n",
    "    p_uncorrected = np.empty(ncomp, dtype=float)\n",
    "    Rmean = R / n\n",
    "\n",
    "    for pp, (ii, jj) in enumerate(to_compare):\n",
    "\n",
    "        # standardized score\n",
    "        Zij = (np.abs(Rmean[ii] - Rmean[jj]) /\n",
    "               np.sqrt((1. / 12.) * N * (N + 1) * (1. / n[ii] + 1. / n[jj])))\n",
    "        Z_pairs[pp] = Zij\n",
    "\n",
    "    # corresponding p-values obtained from upper quantiles of the standard\n",
    "    # normal distribution\n",
    "    p_uncorrected = stats.norm.sf(Z_pairs) * 2.\n",
    "\n",
    "    # correction for multiple comparisons\n",
    "    reject, p_corrected, alphac_sidak, alphac_bonf = multipletests(\n",
    "        p_uncorrected, method=method)\n",
    "\n",
    "\n",
    "    return H, p_omnibus, Z_pairs, p_corrected, reject\n",
    "\n",
    "def calculate_annova(cat_arr, num_arr):\n",
    "    \"\"\"Calculate Kruskal-Wallis anova and Dunn’s post hoc test\"\"\"\n",
    "    temp_df = pd.DataFrame()\n",
    "    temp_df['cat'] = cat_arr\n",
    "    temp_df['num'] = num_arr\n",
    "    temp_df = temp_df.dropna()\n",
    "    \n",
    "    alpha = 0.05\n",
    "    N_samples = df.shape[0]\n",
    "    \n",
    "    # initialize input series, index : group, values : arr of 'num' vals per group\n",
    "    input_ = temp_df.groupby('cat')['num'].apply(list)\n",
    "\n",
    "    H, p_omnibus, Z_pairs, p_corrected, reject = kw_dunn(input_, method='bonf') \n",
    "\n",
    "    max_index = np.argmax(Z_pairs)\n",
    "    Z_max = Z_pairs[max_index]\n",
    "\n",
    "    p_corrected_max = (p_corrected[max_index])\n",
    "    reject_max = (reject[max_index])\n",
    "\n",
    "    #if Kruskal-Wallis analysis of variance gives a significant result, use Dunn’s post hoc test\n",
    "    if p_omnibus >= alpha:\n",
    "        #meaning P-value > α: The differences between the medians are not statistically significant\n",
    "        pval = p_omnibus          # kw\n",
    "    else: \n",
    "        pval = p_corrected_max    #dunns\n",
    "\n",
    "    corr = Z_max / (N_samples)**0.5\n",
    "    Z2_max = (Z_max**2) / N_samples\n",
    "\n",
    "\n",
    "    return corr, pval, Z2_max # effect size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18c30287",
   "metadata": {},
   "source": [
    "## Inspect Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ed4b4c12",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-24T16:59:15.555593Z",
     "start_time": "2023-05-24T16:59:15.550925Z"
    }
   },
   "outputs": [],
   "source": [
    "# read data: mean\n",
    "agg = 'mean'\n",
    "lookback_window = 60\n",
    "source_date = '2022-06-01'\n",
    "\n",
    "glob_path = f'../datasets/per-vehicle-moving-average/{agg}-window-{lookback_window}-{source_date}/*.csv'\n",
    "filepaths = glob(glob_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "21278199",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-24T16:59:15.562479Z",
     "start_time": "2023-05-24T16:59:15.558795Z"
    }
   },
   "outputs": [],
   "source": [
    "def correct_service_rd_kphlimit(temp_df):\n",
    "    temp_df.loc[temp_df['speed_limit_kph'] == 36.7, 'speed_limit_kph'] = 20\n",
    "    return temp_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ffbc8c4",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-05-24T16:59:14.431Z"
    }
   },
   "outputs": [],
   "source": [
    "# get dtypes\n",
    "dtypes_dict = pd.read_csv(filepaths[0]).dtypes.to_dict()\n",
    "\n",
    "# set low_memory=False to ensure no mixed types\n",
    "full_df = pd.concat([pd.read_csv(path, dtype=dtypes_dict) for path in filepaths])\n",
    "full_df = correct_service_rd_kphlimit(full_df)\n",
    "\n",
    "full_df.info()\n",
    "display(full_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99c48767",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-05-24T16:59:14.432Z"
    }
   },
   "outputs": [],
   "source": [
    "# quick eda on some cols\n",
    "def filter_data(df, thresh):\n",
    "    if thresh:\n",
    "        df = df.loc[df['num_periods'] >= thresh]\n",
    "        return df\n",
    "    else:\n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9211d3cc",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-05-24T16:59:14.434Z"
    }
   },
   "outputs": [],
   "source": [
    "# FILTERED\n",
    "thresh = 12\n",
    "original_size = full_df.shape[0]\n",
    "full_df = filter_data(full_df, thresh=thresh)\n",
    "new_size = full_df.shape[0]\n",
    "\n",
    "above_thresh_percentage = (new_size / original_size) * 100\n",
    "print(f\"Percentage of samples with period equal to or over {thresh}: \", above_thresh_percentage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8b4292f",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-05-24T16:59:14.435Z"
    }
   },
   "outputs": [],
   "source": [
    "# use filtered\n",
    "test_size = 0.20 # 20% of data set used for evaluation, 80% as train\n",
    "tuning_size = 0.20 # 20% of the training set is used for tuning\n",
    "\n",
    "\n",
    "retrain, test = train_test_split(full_df, test_size=test_size, random_state=11)\n",
    "_, tune = train_test_split(retrain, test_size=tuning_size, random_state=11)\n",
    "\n",
    "# run_date = str(datetime.now().date()) # now date\n",
    "# print(run_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b895f17",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-05-24T16:59:14.436Z"
    }
   },
   "outputs": [],
   "source": [
    "label_map = {\n",
    "    'hour' : 'Hour',\n",
    "    'month' : 'Month',\n",
    "    'dayofweek' : 'Day of week',\n",
    "    'pix_residential_4x4' : 'Nearby Residential',\n",
    "    'pix_institutional_4x4' : 'Nearby Institutional',\n",
    "    'pix_industrial_4x4' : 'Nearby Industrial',\n",
    "    'pix_business_4x4' : 'Nearby Commercial',\n",
    "    'number_of_lanes' : 'Number of Lanes',\n",
    "    'road_segment_length' : 'Road segment length',\n",
    "    'speed_limit_kph' : 'Speed Limit',\n",
    "    'decel' : 'Deceleration',\n",
    "    'distance_to_road' : 'Vehicle distance\\nto road center',\n",
    "    'accel' : 'Acceleration',\n",
    "    'elevation': 'Elevation (m)',\n",
    "    'agg_speed' : 'Aggregated speed',\n",
    "}\n",
    "\n",
    "categorical_cols = ['hour', 'dayofweek', 'month']\n",
    "numerical_cols = ['pix_residential_4x4', 'pix_business_4x4', 'pix_institutional_4x4', 'pix_industrial_4x4', \n",
    "                  'number_of_lanes', 'road_segment_length', 'speed_limit_kph', 'elevation', 'accel', 'decel', \n",
    "                  'distance_to_road', 'agg_speed']\n",
    "\n",
    "usecols = categorical_cols + numerical_cols\n",
    "df = full_df\n",
    "df = df[usecols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "953395b4",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-05-24T16:59:14.437Z"
    }
   },
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "453b9263",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-05-24T16:59:14.438Z"
    }
   },
   "outputs": [],
   "source": [
    "df.isnull().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b95e8b36",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-05-24T16:59:14.439Z"
    }
   },
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43415594",
   "metadata": {},
   "source": [
    "## Inspect Distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3373b293",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-05-24T16:59:14.441Z"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# _, axes = plt.subplots(4, 3, figsize=(20, 15))\n",
    "fig, axes = plt.subplots(4, 3, figsize=pu.figsizes['double'])\n",
    "\n",
    "flat_axes = axes.flatten()\n",
    "for index, col in enumerate(numerical_cols):\n",
    "    df[col].plot.hist(bins=50, ax=flat_axes[index])\n",
    "    flat_axes[index].set_title(label_map[col], fontsize=pu.tiny)\n",
    "    flat_axes[index].spines[['top', 'right']].set_visible(False)\n",
    "    flat_axes[index].tick_params(axis='both', labelsize=pu.tiny)\n",
    "    ylabel = flat_axes[index].get_ylabel()\n",
    "    flat_axes[index].set_ylabel('')\n",
    "    \n",
    "fig.text(0, 0.5, 'Frequency', va='center', rotation='vertical')\n",
    "plt.tight_layout(pad=1.0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7482f89",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b8b9523",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-05-24T16:59:14.442Z"
    }
   },
   "outputs": [],
   "source": [
    "# feature engineering (removing outliers, impute on nulls)\n",
    "df = df[df.distance_to_road <= 100] # meters away from an edge linestring\n",
    "\n",
    "## these were already filtered prior\n",
    "# df = df[df.accel <= 20]\n",
    "# df = df[df.decel <= 20] \n",
    "# df = df[df.vehicle_speed <= 60] \n",
    "# df = df[df.altitude <= 5000] \n",
    "\n",
    "df = df.dropna(subset=['accel', 'decel'])\n",
    "df['number_of_lanes'] = df['number_of_lanes'].fillna(value=1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93fbacc5",
   "metadata": {},
   "source": [
    "### New distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ded921a",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-05-24T16:59:14.443Z"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# _, axes = plt.subplots(4, 3, figsize=(20, 15))\n",
    "fig, axes = plt.subplots(4, 3, figsize=pu.figsizes['double'])\n",
    "flat_axes = axes.flatten()\n",
    "for index, col in enumerate(numerical_cols):\n",
    "    df[col].plot.hist(bins=50, ax=flat_axes[index])\n",
    "    flat_axes[index].set_title(label_map[col], fontsize=pu.tiny)\n",
    "    flat_axes[index].spines[['top', 'right']].set_visible(False)\n",
    "    flat_axes[index].tick_params(axis='both', labelsize=pu.tiny)\n",
    "    ylabel = flat_axes[index].get_ylabel()\n",
    "    flat_axes[index].set_ylabel('')\n",
    "    \n",
    "fig.text(0, 0.5, 'Frequency', va='center', rotation='vertical')\n",
    "plt.tight_layout(pad=1.0)\n",
    "\n",
    "plt.savefig('../figures/feature_distributions_FULL.pdf', bbox_inches='tight')\n",
    "# plt.savefig('../figures/feature_distributions_FILTERED.pdf', bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66667911",
   "metadata": {},
   "source": [
    "## Calculate Effect Sizes, Correlations, and p-values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b6e719f",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-05-24T16:59:14.445Z"
    }
   },
   "outputs": [],
   "source": [
    "features = list(sorted(set(df.columns) - set(['agg_speed'])))\n",
    "feature_combinations = list(combinations(features, 2))\n",
    "\n",
    "effect_size_df = pd.DataFrame(index=usecols, columns=usecols)\n",
    "corr_df = pd.DataFrame(index=usecols, columns=usecols)\n",
    "pval_df = pd.DataFrame(index=usecols, columns=usecols)\n",
    "\n",
    "for feat1, feat2 in tqdm(feature_combinations):\n",
    "    if (feat1 in categorical_cols) & (feat2 in categorical_cols): # chi2-cramersv\n",
    "        arr1, arr2 = df[feat1], df[feat2]\n",
    "        corr, pval, dof, expected, row_count, col_count, n = calculate_chi(arr1, arr2)\n",
    "        \n",
    "        # corrected cramersv, corr is chi2\n",
    "        effect_size = bergsma_corrected_cramers_v2(corr, row_count, col_count, n)\n",
    "    \n",
    "    elif (feat1 in numerical_cols) & (feat2 in numerical_cols): # spearmanr\n",
    "        arr1, arr2 = df[feat1], df[feat2]\n",
    "        corr, pval, effect_size = calculate_spearmanr(arr1, arr2)\n",
    "    \n",
    "    else:\n",
    "        if feat1 in categorical_cols: # feat1 is cat\n",
    "            # Z2_max : effect_size\n",
    "            cat_arr = df[feat1]\n",
    "            num_arr = df[feat2]\n",
    "        else: # feat2 is cat\n",
    "            cat_arr = df[feat2]\n",
    "            num_arr = df[feat1]\n",
    "            \n",
    "        corr, pval, effect_size = calculate_annova(cat_arr, num_arr)\n",
    "    \n",
    "    effect_size_df.loc[feat1, feat2] = effect_size\n",
    "    corr_df.loc[feat1, feat2] = corr\n",
    "    pval_df.loc[feat1, feat2] = pval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02ee2a9e",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-05-24T16:59:14.447Z"
    }
   },
   "outputs": [],
   "source": [
    "effect_size_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12f8533e",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-05-24T16:59:14.450Z"
    }
   },
   "outputs": [],
   "source": [
    "pval_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e7984d2",
   "metadata": {},
   "source": [
    "## Inspect Effect Size Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3302e982",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-05-24T16:59:14.451Z"
    }
   },
   "outputs": [],
   "source": [
    "edge_list = pd.concat([effect_size_df.stack(), pval_df.stack()], axis=1)\n",
    "edge_list = edge_list.reset_index()\n",
    "edge_list.columns = ['source', 'target', 'effect_size', 'pval']\n",
    "edge_list.info()\n",
    "# edge_list = edge_list[edge_list.pval <= 0.05] # usual alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67777894",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-05-24T16:59:14.452Z"
    }
   },
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "G = nx.from_pandas_edgelist(edge_list,  edge_attr=True)\n",
    "fig, ax = plt.subplots(1, 1, figsize=(10, 10))\n",
    "\n",
    "weights = list(nx.get_edge_attributes(G,'effect_size').values())\n",
    "# nx.draw_networkx(G) \n",
    "nx.draw_networkx(G, width=weights) # weighted according to effect size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7704e9d7",
   "metadata": {},
   "source": [
    "## Generate Correlation Heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e7a1dea",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-05-24T16:59:14.454Z"
    }
   },
   "outputs": [],
   "source": [
    "target_var = 'agg_speed'\n",
    "adj_mat = nx.adjacency_matrix(G, weight='effect_size').todense()\n",
    "adj_df = pd.DataFrame(adj_mat, columns=G.nodes, index=G.nodes)\n",
    "np.fill_diagonal(adj_df.values, 1.0)\n",
    "\n",
    "# renames for plotting\n",
    "names =[label_map[node] for node in G.nodes] \n",
    "\n",
    "adj_df.columns = names\n",
    "adj_df.index = names\n",
    "\n",
    "adj_df = adj_df.reindex([label_map[col] for col in usecols if col != target_var])\n",
    "adj_df = adj_df[[label_map[col] for col in usecols if col != target_var]]\n",
    "adj_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a3aaa8a",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-05-24T16:59:14.455Z"
    }
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "# Generate a mask for the upper triangle\n",
    "mask = np.triu(m=np.ones_like(adj_df, dtype=bool))\n",
    "\n",
    "# Set up the matplotlib figure\n",
    "# f, ax = plt.subplots(figsize=(15, 15))\n",
    "f, ax = plt.subplots(figsize=pu.figsizes['double'])\n",
    "\n",
    "# Generate a custom diverging colormap\n",
    "cmap = sns.diverging_palette(230, 20, as_cmap=True)\n",
    "\n",
    "sns.set_context(\"paper\", rc={\"font.size\":10,\"axes.titlesize\":10,\"axes.labelsize\":15})   \n",
    "\n",
    "ax.tick_params(axis='both', left=False, bottom=False, labelsize=pu.tiny)\n",
    "\n",
    "# Draw the heatmap with the mask and correct aspect ratio\n",
    "sns.heatmap(adj_df, \n",
    "            mask=mask, \n",
    "            cmap=cmap, \n",
    "            annot=True, fmt='.4f', annot_kws={\"size\": 4},\n",
    "            vmax=1.0, vmin=0.0, center=0,\n",
    "            square=True, \n",
    "            linewidths=1.0, \n",
    "            cbar_kws={\n",
    "                \"shrink\": 0.75, 'label':'Effect size', 'pad' : -0.1,\n",
    "            },\n",
    "            ax=ax)\n",
    "\n",
    "xlabels = ax.get_xticklabels()\n",
    "ylabels = ax.get_yticklabels()\n",
    "xlabels[-1] = ylabels[0] = \"\" # remove 1st and last labels\n",
    "\n",
    "ax.set_yticklabels(ylabels)\n",
    "ax.set_xticklabels(xlabels)\n",
    "\n",
    "ax.collections[0].colorbar.outline.set_color(\"black\")\n",
    "ax.collections[0].colorbar.outline.set_linewidth(0.5)\n",
    "ax.collections[0].colorbar.ax.tick_params(labelsize=pu.tiny, width=0.5)\n",
    "cbar_ylabel = ax.collections[0].colorbar.ax.get_ylabel()\n",
    "ax.collections[0].colorbar.ax.set_ylabel(cbar_ylabel, fontsize=pu.label_size)\n",
    "\n",
    "# ax.set_title('Effect Size Heatmap of Features', fontsize=20)\n",
    "\n",
    "plt.savefig('../figures/effect_size_heatmap_FULL.pdf', bbox_inches='tight')\n",
    "# plt.savefig('../figures/effect_size_heatmap_FILTERED.pdf', bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3fd48b4",
   "metadata": {},
   "source": [
    "## End"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python [conda env:root] *",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
